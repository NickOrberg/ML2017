\documentclass[a4paper,12pt]{article}
\input{./base}

\begin{document}
  \section*{Вопросы к экзамену}
  
  \subsection*{Основные определения}  
  
  \myparagraph Основные определения в машинном обучении: объект, целевая функция, признак, модель, обучающая выборка, функционал качества, обучение, переобучение. Приведите примеры задач обучения. 

  \myparagraph Задачи машинного обучения - обучение с учителем, без учителя. Задачи регрессии и классификации. Задачи снижения размерности и кластеризации. Приведите пример каждой задачи на практике.

  \myparagraph Типы признаков в машинном обучении. Приведите примеры различных признаков.
  
  \myparagraph Что такое ROC-кривая, как она определяется? Как она эффективно вычисляется?
  
  \subsection*{Метрические классификаторы}  
  
  \myparagraph Метод $k$ ближайших соседей в задаче классификации. 

  \myparagraph Методы отбора признаков. Жадный метод.  

  \myparagraph Как определяется понятие отступа в метрических алгоритмах классификации? Алгоритм Condensed Nearest Neighbor. Зачем нужен отбор опорных объектов в метрических алгоритмах классификации?
  
  \myparagraph Метод $k$ ближайших соседей в задаче регрессии.
  
  \myparagraph Обобщение метода $k$ ближайших соседей через взвешенный учет объектов. 
  
  \myparagraph Ядерная оценка плотности. Что такое окно переменной ширины, в каких случаях его стоит использовать?
  
  \myparagraph Проклятие размерности. Почему прогнозы методом ближайших соседей зависят от масштабирования признаков? Привести пример. Способы стандартизации признаков.
  
  \myparagraph Функции расстояния для метрических классификаторов.  
  
  \subsection*{Кластеризация}  
  
  \myparagraph Постановка задачи кластеризации. Гипотеза компактности. Цели кластеризации.
  
  \myparagraph Типы кластерных структур. Чувствительность к нормировке и масштабированию признаков.
  
  \myparagraph Метод $k$ средних. Особенности метода. Степени свободы метода $k$ средних.
  
  \myparagraph Метод $k$-means++. Метод Xmeans. 

  \myparagraph Метод распространения близости.
  
  \myparagraph Графовые алгоритмы кластеризации.
  
  \myparagraph Алгоритм Ланса-Уильямса.
  
  \myparagraph Методы визуализации кластеров.
  
  \myparagraph Обобщение методов кластеризации на случай обучения с частичным привлечением учителя.

  \subsection*{Деревья принятия решений}  

  \myparagraph Что такое логическая закономерность? Приведите примеры закономерностей. Интерпретируемость и информативность.

  \myparagraph Часто используемые типы логических закономерностей.

  \myparagraph Что такое решающий список? Достоинства и недостатки.
  
  \myparagraph Структура решающего дерева, метод спуска по дереву в общем случае. Достоинства и недостатки решающих деревьев.

  \myparagraph Зачем делается подрезание решающих деревьев?
  
  \myparagraph Какие критерии информативности используются при синтезе решающего дерева и почему?   

  \myparagraph Небрежные решающие деревья.
    
  \subsection*{Байесовские методы}
  
  \myparagraph Общая формула байесовского классификатора. Функция правдоподобия и априорная вероятность.
  
  \myparagraph Что такое наивный байесовский классификатор?  

  \myparagraph Восстановление плотности распределения по выборке.
  
  \myparagraph Аддитивное сглаживание для байесова классификатора.
  
  \subsection*{Линейные классификаторы}
  
  \myparagraph Модель МакКаллока-Питтса   
  
  \myparagraph Обобщённая модель линейного классификатора. Определение отступа. Минимизация эмпирического риска.

  \myparagraph Метод градиентного спуска. Выбор величины шага.
  
  \myparagraph $L_2$ регуляризация.

  \myparagraph Метод стохастического градиента. Недостатки метода SG и как с ними бороться.
  
  \myparagraph Что такое «сокращение весов»?

  \subsection*{Способность к обобщению}
  
  \myparagraph Внутренний и внешний функционал качества. Что такое кросс-валидация? Недостатки кросс-валидации и как с ними бороться.  
  
  \myparagraph Качество прогнозирования на обучающем множестве и на контрольном множестве. Как они соотносятся друг с другом? Как они меняются, если мы изменяем сложность модели, и когда мы изменяем число объектов обучающей выборки? Критерий непротиворечивости моделей.

  \myparagraph Аналитическая оценка вероятности переобучения. Схема использования. 
  
  \myparagraph Неравенство Бернштейна-Хёфдинга в применении к задаче выбора модели.
  
  \myparagraph Дихотомии. Функция роста. Точка разрыва.
  
  \myparagraph Оценка на максимальное число дихотомий.
  
  \subsection*{Нейронные сети}
  
  \myparagraph Представимость функций в виде нейросети.
  
  \myparagraph Метод обратного распространения ошибок. Основные недостатки и способы их устранения.

  \myparagraph Как можно выбирать начальное приближение в градиентных методах настройки нейронных сетей? Функции активации.

  \myparagraph Устройство свёрточной нейросети.
  
  \subsection*{Метод опорных векторов}
  
  \myparagraph Постановка задачи SVM. Какая функция потерь используется в SVM? 
  
  \myparagraph Регуляризация в задаче SVM. 
  
  \myparagraph Ядро в алгоритме SVM. Зачем вводятся ядра? 
  
  \myparagraph Двойственная задача SVM. 

  \myparagraph Представление метода опорных векторов в виде нейронной сети.
  
  \myparagraph Метод SVR для задачи регрессии.
      
  \subsection*{Линейная регрессия}
  
  \myparagraph Постановка задачи многомерной линейной регрессии. Матричная запись.
  
  \myparagraph Что такое сингулярное разложение? Как оно используется для решения задачи наименьших квадратов?

  \myparagraph Проблема «мультиколлинеарности» в задачах многомерной линейной регрессии. 
  
  \myparagraph Гребневая регрессия. Регуляризация Лассо.
  
  \myparagraph Нелинейная регрессия. Метод Ньютона-Гаусса.
  
  \myparagraph Задача уменьшения размерности. Метод главных компонент. 
  
  \subsection*{Анализ смещения и разброса}
  
  \myparagraph 
  
  \myparagraph 
  
  \myparagraph 
  
  \myparagraph 
  
  \myparagraph 
  
  \subsection*{Ансамбли}
  
  \myparagraph Комбинирование слабых решающих правил. Бустинг. Алгоритм AdaBoost. 
  
  \myparagraph 
  
  \myparagraph 
  
  \myparagraph 
  
  \myparagraph 
\end{document}
