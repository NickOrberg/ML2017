\documentclass[10pt]{beamer}
\input{../base}

\title{Лекция 13}
\subtitle{Композиции алгоритмов}

\begin{document}

\section{Разбор летучки}

\maketitle

\section{Мотивация}

\section{Где уже видели композиции?}

\begin{frame}{Постановка задачи}
  ${X^l = (x_i, y_i)_{i = 1}^l}$ -- обучающая выборка\\
  $b:X \rightarrow R$ --- базовый алгоритм\\
  $C:R \rightarrow Y$ --- решающее правило\\
  $R$ --- пространство оценок.
  \bigbreak
  Искомый алгоритм: $a(x) = C(b(x))$\\
\end{frame}

\begin{frame}{Определение композиции}
  Композиция базовых алгоритмов $b_1, \dots, b_T$\\
  \bigbreak
  $a(x) = C(F(b_1(x), \dots, b_T(x)))$\\
  $F: R^T \rightarrow R$ -- корректирующая операция
\end{frame}

\begin{frame}{Примеры}
  \begin{enumerate}
    \item Простое голосование\\
      $$F(b_1(x), \dots, b_T(x)) = \sum\limits_{t=1}^{T} b_t(x)$$
    \item Взвешенное голосование\\
      $$F(b_1(x), \dots, b_T(x)) = \sum\limits_{t=1}^{T} \alpha_t b_t(x)$$
    \item Смесь алгоритмов\\
      $$F(b_1(x), \dots, b_T(x)) = \sum\limits_{t=1}^{T} g_t(x) b_t(x)$$      
  \end{enumerate}
\end{frame}

\section{Простое голосование}

\begin{frame}{Теорема Кондорсе "о жюри присяжных"}
  Если каждый член жюри присяжных имеет независимое мнение, и если вероятность правильного решения члена жюри больше 0.5, то тогда вероятность правильного решения присяжных в целом возрастает с увеличением количества членов жюри и стремится к единице.   
\end{frame}


\begin{frame}{Простое голосование}
  $$F(b_1(x), \dots, b_T(x)) = \sum\limits_{t=1}^{T} b_t(x)$$
  \bigbreak
  \pause
  $$a(x) = \sign \left( \sum\limits_{t=1}^{T} b_t(x) \right)$$
  \bigbreak
  \pause
  Если каждый из $b_t$ лучше случайного гадания и $b_1, \dots, b_T$ достаточно различны, то композиция может работать лучше.
\end{frame}

\begin{frame}{Достаточно различны?}
  \begin{itemize}
    \item Настройка по случайным подвыборкам
    \item Обучение по случайным подмножествам признаков
    \item Использование различных начальных приближений 
    \item Использование различных моделей
  \end{itemize}
\end{frame}

{\foot{Bagging, Bootstrap aggregating}
\begin{frame}{Бэггинг}
    \alert{Идея}: обучим $b_t$ независимо по случайным подвыборкам длины $l$ с повторениями.\\
    Доля объектов, которые попадут в выборку $\approx 0.63$
\end{frame}
}

{\foot{RSM, Random Subspace Method}
\begin{frame}{Метод случайных подпространств}
    \alert{Идея}: обучим $b_t$ независимо по случайным подмножествам $n'$ признаков.\\
\end{frame}
}

\begin{frame}{Алгоритм}
  	\begin{algorithmic}[1]
    \Function{bagging\_rsm}{$X^l$, $T$, $l'$, $n'$, $\varepsilon_1$, $\varepsilon_2$}
      \For {$t = 1, \dots, T$}
        \State $U_t$ -- случайное подмножество $X^l$ длины $l'$
        \State $F_t$ -- случайное подмножество признаков длины $n'$
        \State $b_t = \mu (F_t, U_t)$
        \If {$Q(b_t, U_t) > \varepsilon_1$ или $Q(b_t, X^l \setminus U_t) > \varepsilon_2$}
          \State не включать $b_t$ в композициюd
        \EndIf
      \EndFor
    \EndFunction
  \end{algorithmic}
\end{frame}

{\foot{Random forest}
\begin{frame}{Случайный лес}
  Бэггинг над решающими деревьями.\\
  \bigbreak
	Голосование деревьев классификации, $Y = \left\{ -1, +1 \right\}$\\
	$a(t) = Majority(b_t(x))$%\sign \frac{1}{T} \sum\limits_{t=1}^T b_t(x)$\\
	\begin{enumerate}[--]
  	  \item Каждое дерево $b_t(x)$ обучается по случайной выборке с повторениями
	  \item В каждой вершине предикат выбирается из случайного подмножества $n$ предикатов
	\end{enumerate}
\end{frame}
}

\section{Взвешенное голосование}

{\foot{Boosting}
\begin{frame}{Бустинг}
  $Y = \{\pm 1\}$, \qquad $b_t: X\rightarrow \{-1, 0, +1\}$, \qquad $C(b) = \sign(b)$\\
  $b_t(x) = 0$ -- отказ от классификации\\
  \bigbreak
  \pause
  $a(x) = \sign\left(\sum\limits_{t=1}^{T} \alpha_t b_t(x)\right)$\\
  \bigbreak
  Функционал качества композиции:\\
  $Q_T = \sum\limits_{i=1}^l \mathcal{L}(\sum\limits_{t=1}^{T} \alpha_t b_t(x), y_i) \rightarrow \min\limits_{\alpha, b}$
\end{frame}
}

{\foot{Boosting}
\begin{frame}{Бустинг}
  \alert{Идея}: Фиксируем $\alpha_1 b_1(x) \dots \alpha_{t-1} b_{t-1}(x)$ при добавлении $b_t$
  \bigbreak
  \pause
  $b_1 = \arg\min\limits_{b} Q(b, X^l)$\\
  $b_2 = \arg\min\limits_{b, F} Q(F(b_1, b), X^l)$\\
  $\dots$\\
  $b_t = \arg\min\limits_{b, F} Q(F(b_1, \dots, b_{t-1}, b), X^l)$
\end{frame}
}

\begin{frame}{Adaboost}
  $B$ -- семейство базовых алгоритмов.\\
  \bigbreak
  Пусть для любого нормированного вектора весов $U^l$ существует алгоритм $b \in B$, классифицирующий выборку немного лучше, чем наугад $P(b, U^l) > N (b, U^l)$. 
  Минимум функционала $Q_T$ достигается при:\\
  $$b_T = \arg\max\limits_{b \in B} \sqrt{P(b, W^l)} - \sqrt{N(b, W^l)}$$
  $$a_T = \frac{1}{2} \ln \frac{P(b_T, W^l)}{N(b_T, W^l)}$$
\end{frame}

\begin{frame}{Рекоммендации}
  \begin{itemize}
    \item Чаще всего в качестве базовых классификаторов используются решающие деревья
    \item Для SVM бустинг не эффективен     
  \end{itemize}
\end{frame}

\begin{frame}{Обоснование бустинга}
    
\end{frame}

\begin{frame}{Наблюдения}
  \begin{itemize}[<+->]
    \item[+] Хорошая обобщающая способность
    \item[+] Простота реализации
    \item[+] Накладные расходы на построение не велики
    \bigbreak
    \item[--] Склонен к переобучению при наличии большого количества шума в данных
    \item[--] Требует большой обучающей выборки
    \item[--] Жадность приводит к неоптимальности
    
  \end{itemize}
\end{frame}

\begin{frame}{Бэггинг}
    
\end{frame}

\begin{frame}{Bagboo}
    
\end{frame}

\begin{frame}{Stacking}
    \centering
    \includegraphics[width=0.9 \textwidth, keepaspectratio]{images/stacking}
\end{frame}

\begin{frame}{Почему эти подходы работают}
  \begin{enumerate}
    \item Бэггинг уменьшает разброс
    \item Бустинг уменьшает разброс и смещение
    \item Чем сильнее коррелируют базовые алгоритмы, тем менее эффективны композиции
  \end{enumerate}
\end{frame}

\begin{frame}[standout]
  Вопросы?
\end{frame}

\appendix

\end{document}

\end{document}